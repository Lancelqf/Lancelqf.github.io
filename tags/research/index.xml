<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Research on Qingfeng&#39;s blog</title>
    <link>https://lancelqf.github.io/tags/research/</link>
    <description>Recent content in Research on Qingfeng&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Apr 2019 16:01:41 -0600</lastBuildDate><atom:link href="https://lancelqf.github.io/tags/research/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Lifelong Learning</title>
      <link>https://lancelqf.github.io/note/lifelong_learning/</link>
      <pubDate>Fri, 26 Apr 2019 16:01:41 -0600</pubDate>
      
      <guid>https://lancelqf.github.io/note/lifelong_learning/</guid>
      <description>Introduction: Definition of LL Lifelong learning (LL) is a continuous learning process. At any point in time, the learner has performed a sequence of $N$ learning tasks, $T _ { 1 } , T _ { 2 } , \ldots , T _ { N }$. These tasks have their corresponding datasets $D _ { 1 } , D _ { 2 } , \ldots , D _ { N }$.</description>
    </item>
    
    <item>
      <title>Deep Reinforcement Learning</title>
      <link>https://lancelqf.github.io/note/deep_rl/</link>
      <pubDate>Fri, 26 Apr 2019 15:50:46 -0600</pubDate>
      
      <guid>https://lancelqf.github.io/note/deep_rl/</guid>
      <description>Value-based methods for deep RL   Q-learning:
 The Q-learning algorithm uses Bellman equation to get the unique solution $Q^{*}(s,a)$: $$Q ^ { * } ( s , a ) = ( \mathcal { B } Q ^ { * } ) ( s , a )$$ where $\mathcal{B}$ is the Bellman operator mapping any function $K : \mathcal { S } \times \mathcal { A } arrow \mathbb { R }$ into another function $\mathcal { S } \times \mathcal { A } arrow \mathbb { R }$, defined as follows: $$( \mathcal { B } K ) ( s , a ) = \sum _ { s ^ { \prime } \in S } P ( s , a , s ^ { \prime } ) ( R ( s , a , s ^ { \prime } ) + \gamma \max _ { a ^ { \prime } \in \mathcal { A } } K ( s ^ { \prime } , a ^ { \prime } ) )$$ By Banach&amp;rsquo;s theorem, the fixed point of the Bellman operator $\mathcal{B}$ exists since it is a contraction mapping.</description>
    </item>
    
    <item>
      <title>Transfer Learning</title>
      <link>https://lancelqf.github.io/note/transfer_learning/</link>
      <pubDate>Fri, 26 Apr 2019 15:42:36 -0600</pubDate>
      
      <guid>https://lancelqf.github.io/note/transfer_learning/</guid>
      <description>Introduction I&amp;rsquo;ve run out of my ideas. Time to learn new things. I start with transfer learning. After that, I&amp;rsquo;d probably continue with online learning, life-long learning and meta learning.
For transfer learning, there are two important concepts:
 Domain: A domain is consists of data and data distribution. To be specific, there are two domains that we care in transfer learning: Source Domain ($D _ s$) and Target Domain ($D _ t$).</description>
    </item>
    
    <item>
      <title>Research Diary 2019</title>
      <link>https://lancelqf.github.io/note/research_diary_2019/</link>
      <pubDate>Mon, 21 Jan 2019 18:40:42 -0700</pubDate>
      
      <guid>https://lancelqf.github.io/note/research_diary_2019/</guid>
      <description>This is my research diary about intelligence in 2019. I find writing research diary really interesting and it helps me to organize my thougths and discover new ideas! I just hope I have enough thougths to continue writing.
Reference  Reinforcement Learning: An Introduction. Richard S. Sutton and Andrew G. Barto, Second Edition, MIT Press, Cambridge, MA, 2018. Artificial Intelligence: A Modern Approach. Stuart Russell and Peter Norvig, Third Edition, Prentice Hall, 2009.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://lancelqf.github.io/about/</link>
      <pubDate>Tue, 03 Apr 2018 19:12:33 +0800</pubDate>
      
      <guid>https://lancelqf.github.io/about/</guid>
      <description>Biography My name is Qingfeng Lan, a PhD student at the University of Alberta, Canada. I&amp;rsquo;m interested in designing simple and efficient algorithms supported by sound theories and verified by rigorous experiments. In particular, my research focuses on reinforcement learning and representation learning. Please see my CV for more information.
Contact I&amp;rsquo;d like to chat to people with different research/social/cultural/educational backgrounds, as windows to see the unknown world. Feel free to contact me if you want to share anything (ideas, experiences, etc.</description>
    </item>
    
  </channel>
</rss>
