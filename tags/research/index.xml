<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Research on Qingfeng&#39;s blog</title>
    <link>https://lancelqf.github.io/tags/research/</link>
    <description>Recent content in Research on Qingfeng&#39;s blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Mar 2025 01:08:34 -0600</lastBuildDate>
    <atom:link href="https://lancelqf.github.io/tags/research/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>From REINFORCE to Dr. GRPO</title>
      <link>https://lancelqf.github.io/note/llm_post_training/</link>
      <pubDate>Thu, 27 Mar 2025 01:08:34 -0600</pubDate>
      <guid>https://lancelqf.github.io/note/llm_post_training/</guid>
      <description>&lt;p&gt;Recently, many reinforcement learning (RL) algorithms have been applied to improve the post-training of large language models (LLMs). In this article, we aim to provide a unified perspective on the objectives of these RL algorithms, exploring how they relate to each other through the Policy Gradient Theorem [1] â€” the fundamental theorem of policy gradient methods.&lt;/p&gt;&#xA;&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;&#xA;&lt;p&gt;Let $\Delta(X)$ be the space of all probability distributions supported over the set $X$.&#xA;Consider a Markov decision process (MDP), $M=(\mathcal{S}, \mathcal{A}, \mathbb{P}, p_0, R, \gamma)$, where $\mathcal{S}$ is the discrete state space, $\mathcal{A}$ is the discrete action space, $\mathbb{P}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})$ is the transition probability, $p_0 \in \Delta(\mathcal{S})$ is the initial state distribution, $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is the reward function, and $\gamma \in [0,1]$ is the discount factor.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lifelong Learning</title>
      <link>https://lancelqf.github.io/note/lifelong_learning/</link>
      <pubDate>Fri, 26 Apr 2019 16:01:41 -0600</pubDate>
      <guid>https://lancelqf.github.io/note/lifelong_learning/</guid>
      <description></description>
    </item>
    <item>
      <title>Deep Reinforcement Learning</title>
      <link>https://lancelqf.github.io/note/deep_rl/</link>
      <pubDate>Fri, 26 Apr 2019 15:50:46 -0600</pubDate>
      <guid>https://lancelqf.github.io/note/deep_rl/</guid>
      <description></description>
    </item>
    <item>
      <title>Transfer Learning</title>
      <link>https://lancelqf.github.io/note/transfer_learning/</link>
      <pubDate>Fri, 26 Apr 2019 15:42:36 -0600</pubDate>
      <guid>https://lancelqf.github.io/note/transfer_learning/</guid>
      <description></description>
    </item>
    <item>
      <title>Research Diary 2019</title>
      <link>https://lancelqf.github.io/note/research_diary_2019/</link>
      <pubDate>Mon, 21 Jan 2019 18:40:42 -0700</pubDate>
      <guid>https://lancelqf.github.io/note/research_diary_2019/</guid>
      <description>&lt;p&gt;This is my research diary about intelligence in 2019. I find writing research diary really interesting and it helps me to organize my thougths and discover new ideas! I just hope I have enough thougths to continue writing.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
