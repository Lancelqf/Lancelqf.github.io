<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Notes on Qingfeng&#39;s blog</title>
    <link>https://lancelqf.github.io/note/</link>
    <description>Recent content in Notes on Qingfeng&#39;s blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Mar 2025 01:08:34 -0600</lastBuildDate>
    <atom:link href="https://lancelqf.github.io/note/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>From REINFORCE to Dr. GRPO</title>
      <link>https://lancelqf.github.io/note/llm_post_training/</link>
      <pubDate>Thu, 27 Mar 2025 01:08:34 -0600</pubDate>
      <guid>https://lancelqf.github.io/note/llm_post_training/</guid>
      <description>&lt;p&gt;Recently, many reinforcement learning (RL) algorithms have been applied to improve the post-training of large language models (LLMs). In this article, we aim to provide a unified perspective on the objectives of these RL algorithms, exploring how they relate to each other through the Policy Gradient Theorem [1] — the fundamental theorem of policy gradient methods.&lt;/p&gt;&#xA;&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;&#xA;&lt;p&gt;Let $\mathcal{M}_1(X)$ be the space of probability distributions supported on the set $X$.&#xA;Consider a Markov decision process (MDP), $M=(\mathcal{S}, \mathcal{A}, \mathbb{P}, p_0, R, \gamma)$, where $\mathcal{S}$ is the discrete state space, $\mathcal{A}$ is the discrete action space, $\mathbb{P}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{M}_1(\mathcal{S})$ is the transition probability, $p_0 = \mathcal{M}_1(\mathcal{S})$ is the initial state distribution, $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is the reward function, and $\gamma \in [0,1]$ is the discount factor.&lt;/p&gt;</description>
    </item>
    <item>
      <title>毛发与指甲护理</title>
      <link>https://lancelqf.github.io/note/%E6%AF%9B%E5%8F%91%E4%B8%8E%E6%8C%87%E7%94%B2%E6%8A%A4%E7%90%86/</link>
      <pubDate>Sun, 27 Jun 2021 12:13:31 -0600</pubDate>
      <guid>https://lancelqf.github.io/note/%E6%AF%9B%E5%8F%91%E4%B8%8E%E6%8C%87%E7%94%B2%E6%8A%A4%E7%90%86/</guid>
      <description></description>
    </item>
    <item>
      <title>皮肤护理</title>
      <link>https://lancelqf.github.io/note/%E7%9A%AE%E8%82%A4%E6%8A%A4%E7%90%86/</link>
      <pubDate>Sat, 26 Jun 2021 20:32:45 -0600</pubDate>
      <guid>https://lancelqf.github.io/note/%E7%9A%AE%E8%82%A4%E6%8A%A4%E7%90%86/</guid>
      <description></description>
    </item>
    <item>
      <title>膳食管理</title>
      <link>https://lancelqf.github.io/note/%E8%86%B3%E9%A3%9F%E7%AE%A1%E7%90%86/</link>
      <pubDate>Wed, 10 Mar 2021 23:20:20 -0700</pubDate>
      <guid>https://lancelqf.github.io/note/%E8%86%B3%E9%A3%9F%E7%AE%A1%E7%90%86/</guid>
      <description></description>
    </item>
    <item>
      <title>Lifelong Learning</title>
      <link>https://lancelqf.github.io/note/lifelong_learning/</link>
      <pubDate>Fri, 26 Apr 2019 16:01:41 -0600</pubDate>
      <guid>https://lancelqf.github.io/note/lifelong_learning/</guid>
      <description></description>
    </item>
    <item>
      <title>Deep Reinforcement Learning</title>
      <link>https://lancelqf.github.io/note/deep_rl/</link>
      <pubDate>Fri, 26 Apr 2019 15:50:46 -0600</pubDate>
      <guid>https://lancelqf.github.io/note/deep_rl/</guid>
      <description></description>
    </item>
    <item>
      <title>Transfer Learning</title>
      <link>https://lancelqf.github.io/note/transfer_learning/</link>
      <pubDate>Fri, 26 Apr 2019 15:42:36 -0600</pubDate>
      <guid>https://lancelqf.github.io/note/transfer_learning/</guid>
      <description></description>
    </item>
    <item>
      <title>Research Diary 2019</title>
      <link>https://lancelqf.github.io/note/research_diary_2019/</link>
      <pubDate>Mon, 21 Jan 2019 18:40:42 -0700</pubDate>
      <guid>https://lancelqf.github.io/note/research_diary_2019/</guid>
      <description>&lt;p&gt;This is my research diary about intelligence in 2019. I find writing research diary really interesting and it helps me to organize my thougths and discover new ideas! I just hope I have enough thougths to continue writing.&lt;/p&gt;</description>
    </item>
    <item>
      <title>个人理财</title>
      <link>https://lancelqf.github.io/note/%E4%B8%AA%E4%BA%BA%E7%90%86%E8%B4%A2/</link>
      <pubDate>Mon, 24 Dec 2018 12:53:44 -0700</pubDate>
      <guid>https://lancelqf.github.io/note/%E4%B8%AA%E4%BA%BA%E7%90%86%E8%B4%A2/</guid>
      <description></description>
    </item>
  </channel>
</rss>
